{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PostgreSQL schema and functions for Spatio-Temporal Asset Catalog (STAC) Documentation : stac-utils.github.io/pgstac/ Source Code : stac-utils/pgstac PgSTAC is a set of SQL function and schema to build highly performant database for Spatio-Temporal Asset Catalog (STAC). The project also provide pypgstac python module to help with the database migration and documents ingestion (collections and items). PgSTAC Documentation: stac-utils.github.io/pgstac/pgstac pyPgSTAC Documentation: stac-utils.github.io/pgstac/pypgstac Project structure \u00b6 / \u251c\u2500\u2500 pypgstac/ - pyPgSTAC python module \u251c\u2500\u2500 scripts/ - scripts to set up the environment \u251c\u2500\u2500 sql/ - PgSTAC SQL code \u2514\u2500\u2500 test/ - test suite Contribution & Development \u00b6 See CONTRIBUTING.md License \u00b6 See LICENSE Authors \u00b6 See contributors for a listing of individual contributors. Changes \u00b6 See CHANGELOG.md .","title":"Home"},{"location":"#project-structure","text":"/ \u251c\u2500\u2500 pypgstac/ - pyPgSTAC python module \u251c\u2500\u2500 scripts/ - scripts to set up the environment \u251c\u2500\u2500 sql/ - PgSTAC SQL code \u2514\u2500\u2500 test/ - test suite","title":"Project structure"},{"location":"#contribution-development","text":"See CONTRIBUTING.md","title":"Contribution &amp; Development"},{"location":"#license","text":"See LICENSE","title":"License"},{"location":"#authors","text":"See contributors for a listing of individual contributors.","title":"Authors"},{"location":"#changes","text":"See CHANGELOG.md .","title":"Changes"},{"location":"contributing/","text":"Development - Contributing \u00b6 PGStac uses a dockerized development environment. However, it still needs a local install of pypgstac to allow an editable install inside the docker container. This is installed automatically if you have set up a virtual environment for the project. Otherwise you'll need to install a local copy yourself by running scripts/install . To build the docker images and set up the test database, use: scripts/setup To bring up the development database: scripts/server To run tests, use: scripts/test To rebuild docker images: scripts/update To drop into a console, use scripts/console To drop into a psql console on the database container, use: scripts/console --db To run migrations on the development database, use scripts/migrate To stage code and configurations and create template migrations for a version release, use scripts/stageversion [ version ] Examples: scripts/stageversion 0.2.8 This will create a base migration for the new version and will create incremental migrations between any existing base migrations. The incremental migrations that are automatically generated by this script will have the extension \".staged\" on the file. You must manually review (and make any modifications necessary) this file and remove the \".staged\" extension to enable the migration. Making Changes to SQL \u00b6 All changes to SQL should only be made in the /sql directory. SQL Files will be run in alphabetical order. Adding Tests \u00b6 PGStac uses PGTap to test SQL. Tests can be found in tests/pgtap.sql and are run using scripts/test Release Process \u00b6 1) Make sure all your code is added and committed 2) Create a PR against the main branch 3) Once the PR has been merged, start the release process. 4) Upate the version in pypgstac/pypgstac/version.py 5) Use scripts/stageversion VERSION as documented in migrations section above making sure to rename any files ending in \".staged\" in the migrations section 6) Add details for release to the CHANGELOG 7) Add/Commit any changes 8) Run tests scripts/test 9) Create a git tag git tag v0.2.8 using new version number 10) Push the git tag git push origin v0.2.8 11) The CI process will push pypgstac to PyPi, create a docker image on ghcr.io, and create a release on github. Get Involved \u00b6 Issues and pull requests are more than welcome: github.com/stac-utils/pgstac/issues","title":"Development - Contributing"},{"location":"contributing/#development-contributing","text":"PGStac uses a dockerized development environment. However, it still needs a local install of pypgstac to allow an editable install inside the docker container. This is installed automatically if you have set up a virtual environment for the project. Otherwise you'll need to install a local copy yourself by running scripts/install . To build the docker images and set up the test database, use: scripts/setup To bring up the development database: scripts/server To run tests, use: scripts/test To rebuild docker images: scripts/update To drop into a console, use scripts/console To drop into a psql console on the database container, use: scripts/console --db To run migrations on the development database, use scripts/migrate To stage code and configurations and create template migrations for a version release, use scripts/stageversion [ version ] Examples: scripts/stageversion 0.2.8 This will create a base migration for the new version and will create incremental migrations between any existing base migrations. The incremental migrations that are automatically generated by this script will have the extension \".staged\" on the file. You must manually review (and make any modifications necessary) this file and remove the \".staged\" extension to enable the migration.","title":"Development - Contributing"},{"location":"contributing/#making-changes-to-sql","text":"All changes to SQL should only be made in the /sql directory. SQL Files will be run in alphabetical order.","title":"Making Changes to SQL"},{"location":"contributing/#adding-tests","text":"PGStac uses PGTap to test SQL. Tests can be found in tests/pgtap.sql and are run using scripts/test","title":"Adding Tests"},{"location":"contributing/#release-process","text":"1) Make sure all your code is added and committed 2) Create a PR against the main branch 3) Once the PR has been merged, start the release process. 4) Upate the version in pypgstac/pypgstac/version.py 5) Use scripts/stageversion VERSION as documented in migrations section above making sure to rename any files ending in \".staged\" in the migrations section 6) Add details for release to the CHANGELOG 7) Add/Commit any changes 8) Run tests scripts/test 9) Create a git tag git tag v0.2.8 using new version number 10) Push the git tag git push origin v0.2.8 11) The CI process will push pypgstac to PyPi, create a docker image on ghcr.io, and create a release on github.","title":"Release Process"},{"location":"contributing/#get-involved","text":"Issues and pull requests are more than welcome: github.com/stac-utils/pgstac/issues","title":"Get Involved"},{"location":"pgstac/","text":"PGDatabase Schema and Functions for Storing and Accessing STAC collections and items in PostgreSQL STAC Client that uses PGStac available in STAC-FastAPI PGStac requires Postgresql>=13 and PostGIS>=3 . Best performance will be had using PostGIS>=3.1. PGStac Settings \u00b6 PGStac installs everything into the pgstac schema in the database. This schema must be in the search_path in the postgresql session while using pgstac. PGStac Users \u00b6 The pgstac_admin role is the owner of all the objects within pgstac and should be used when running things such as migrations. The pgstac_ingest role has read/write priviliges on all tables and should be used for data ingest or if using the transactions extension with stac-fastapi-pgstac. The pgstac_read role has read only access to the items and collections, but will still be able to write to the logging tables. You can use the roles either directly and adding a password to them or by granting them to a role you are already using. To use directly: ALTER ROLE pgstac_read LOGIN PASSWORD '<password>' ; To grant pgstac permissions to a current postgresql user: GRANT pgstac_read TO < user > ; PGStac Search Path \u00b6 The search_path can be set at the database level or role level or by setting within the current session. The search_path is already set if you are directly using one of the pgstac users. If you are not logging in directly as one of the pgstac users, you will need to set the search_path by adding it to the search_path of the user you are using: ALTER ROLE < user > SET SEARCH_PATH TO pgstac , public ; setting the search_path on the database: ALTER DATABASE < database > set search_path to pgstac , public ; In psycopg the search_path can be set by passing it as a configuration when creating your connection: kwargs = { \"options\" : \"-c search_path=pgstac,public\" } PGStac Settings Variables \u00b6 There are additional variables that control the settings used for calculating and displaying context (total row count) for a search, as well as a variable to set the filter language (cql-json or cql-json2). The context is \"off\" by default, and the default filter language is set to \"cql2-json\". Variables can be set either by passing them in via the connection options using your connection library, setting them in the pgstac_settings table or by setting them on the Role that is used to log in to the database. Turning \"context\" on can be very expensive on larger databases. Much of what PGStac does is to optimize the search of items sorted by time where only fewer than 10,000 records are returned at a time. It does this by searching for the data in chunks and is able to \"short circuit\" and return as soon as it has the number of records requested. Calculating the context (the total count for a query) requires a scan of all records that match the query parameters and can take a very long time. Settting \"context\" to auto will use database statistics to estimate the number of rows much more quickly, but for some queries, the estimate may be quite a bit off. Example for updating the pgstac_settings table with a new value: INSERT INTO pgstac_settings ( name , value ) VALUES ( 'default-filter-lang' , 'cql-json' ), ( 'context' , 'on' ) ON CONFLICT ON CONSTRAINT pgstac_settings_pkey DO UPDATE SET value = excluded . value ; Alternatively, update the role: ALTER ROLE < username > SET SEARCH_PATH to pgstac , public ; ALTER ROLE < username > SET pgstac . context TO < 'on' , 'off' , 'auto' > ; ALTER ROLE < username > SET pgstac . context_estimated_count TO '<number of estimated rows when in auto mode that when an estimated count is less than will trigger a full count>' ; ALTER ROLE < username > SET pgstac . context_estimated_cost TO '<estimated query cost from explain when in auto mode that when an estimated cost is less than will trigger a full count>' ; ALTER ROLE < username > SET pgstac . context_stats_ttl TO '<an interval string ie \"1 day\" after which pgstac search will force recalculation of it' s estimates >> ' ; Runtime Configurations \u00b6 Runtime configuration of variables can be made with search by passing in configuration in the search json \"conf\" item. Runtime configuration is available for context, context_estimated_count, context_estimated_cost, context_stats_ttl, and nohydrate. The nohydrate conf item returns an unhydrated item bypassing the CPU intensive step of rehydrating data with data from the collection metadata. When using the nohydrate conf, the only fields that are respected in the fields extension are geometry and bbox. SELECT search ( '{\"conf\":{\"nohydrate\"=true}}' ); PGStac Partitioning \u00b6 By default PGStac partitions data by collection (note: this is a change starting with version 0.5.0). Each collection can further be partitioned by either year or month. Partitioning must be set up prior to loading any data! Partitioning can be configured by setting the partition_trunc flag on a collection in the database. UPDATE collections set partition_trunc = 'month' WHERE id = '<collection id>' ; In general, you should aim to keep each partition less than a few hundred thousand rows. Further partitioning (ie setting everything to 'month' when not needed to keep the partitions below a few hundred thousand rows) can be detrimental. PGStac Indexes / Queryables \u00b6 By default, PGStac includes indexes on the id, datetime, collection, geometry, and the eo:cloud_cover property. Further indexing can be added for additional properties globally or only on particular collections by modifications to the queryables table. Currently indexing is the only place the queryables table is used, but in future versions, it will be extended to provide a queryables backend api. To add a new global index across all partitions: INSERT INTO pgstac . queryables ( name , property_wrapper , property_index_type ) VALUES ( < property name > , < property wrapper > , < index type > ); Property wrapper should be one of to_int, to_float, to_tstz, or to_text. The index type should almost always be 'BTREE', but can be any PostgreSQL index type valid for the data type. More indexes is note necessarily better. You should only index the primary fields that are actively being used to search. Adding too many indexes can be very detrimental to performance and ingest speed. If your primary use case is delivering items sorted by datetime and you do not use the context extension, you likely will not need any further indexes.","title":"PgSTAC"},{"location":"pgstac/#pgstac-settings","text":"PGStac installs everything into the pgstac schema in the database. This schema must be in the search_path in the postgresql session while using pgstac.","title":"PGStac Settings"},{"location":"pgstac/#pgstac-users","text":"The pgstac_admin role is the owner of all the objects within pgstac and should be used when running things such as migrations. The pgstac_ingest role has read/write priviliges on all tables and should be used for data ingest or if using the transactions extension with stac-fastapi-pgstac. The pgstac_read role has read only access to the items and collections, but will still be able to write to the logging tables. You can use the roles either directly and adding a password to them or by granting them to a role you are already using. To use directly: ALTER ROLE pgstac_read LOGIN PASSWORD '<password>' ; To grant pgstac permissions to a current postgresql user: GRANT pgstac_read TO < user > ;","title":"PGStac Users"},{"location":"pgstac/#pgstac-search-path","text":"The search_path can be set at the database level or role level or by setting within the current session. The search_path is already set if you are directly using one of the pgstac users. If you are not logging in directly as one of the pgstac users, you will need to set the search_path by adding it to the search_path of the user you are using: ALTER ROLE < user > SET SEARCH_PATH TO pgstac , public ; setting the search_path on the database: ALTER DATABASE < database > set search_path to pgstac , public ; In psycopg the search_path can be set by passing it as a configuration when creating your connection: kwargs = { \"options\" : \"-c search_path=pgstac,public\" }","title":"PGStac Search Path"},{"location":"pgstac/#pgstac-settings-variables","text":"There are additional variables that control the settings used for calculating and displaying context (total row count) for a search, as well as a variable to set the filter language (cql-json or cql-json2). The context is \"off\" by default, and the default filter language is set to \"cql2-json\". Variables can be set either by passing them in via the connection options using your connection library, setting them in the pgstac_settings table or by setting them on the Role that is used to log in to the database. Turning \"context\" on can be very expensive on larger databases. Much of what PGStac does is to optimize the search of items sorted by time where only fewer than 10,000 records are returned at a time. It does this by searching for the data in chunks and is able to \"short circuit\" and return as soon as it has the number of records requested. Calculating the context (the total count for a query) requires a scan of all records that match the query parameters and can take a very long time. Settting \"context\" to auto will use database statistics to estimate the number of rows much more quickly, but for some queries, the estimate may be quite a bit off. Example for updating the pgstac_settings table with a new value: INSERT INTO pgstac_settings ( name , value ) VALUES ( 'default-filter-lang' , 'cql-json' ), ( 'context' , 'on' ) ON CONFLICT ON CONSTRAINT pgstac_settings_pkey DO UPDATE SET value = excluded . value ; Alternatively, update the role: ALTER ROLE < username > SET SEARCH_PATH to pgstac , public ; ALTER ROLE < username > SET pgstac . context TO < 'on' , 'off' , 'auto' > ; ALTER ROLE < username > SET pgstac . context_estimated_count TO '<number of estimated rows when in auto mode that when an estimated count is less than will trigger a full count>' ; ALTER ROLE < username > SET pgstac . context_estimated_cost TO '<estimated query cost from explain when in auto mode that when an estimated cost is less than will trigger a full count>' ; ALTER ROLE < username > SET pgstac . context_stats_ttl TO '<an interval string ie \"1 day\" after which pgstac search will force recalculation of it' s estimates >> ' ;","title":"PGStac Settings Variables"},{"location":"pgstac/#runtime-configurations","text":"Runtime configuration of variables can be made with search by passing in configuration in the search json \"conf\" item. Runtime configuration is available for context, context_estimated_count, context_estimated_cost, context_stats_ttl, and nohydrate. The nohydrate conf item returns an unhydrated item bypassing the CPU intensive step of rehydrating data with data from the collection metadata. When using the nohydrate conf, the only fields that are respected in the fields extension are geometry and bbox. SELECT search ( '{\"conf\":{\"nohydrate\"=true}}' );","title":"Runtime Configurations"},{"location":"pgstac/#pgstac-partitioning","text":"By default PGStac partitions data by collection (note: this is a change starting with version 0.5.0). Each collection can further be partitioned by either year or month. Partitioning must be set up prior to loading any data! Partitioning can be configured by setting the partition_trunc flag on a collection in the database. UPDATE collections set partition_trunc = 'month' WHERE id = '<collection id>' ; In general, you should aim to keep each partition less than a few hundred thousand rows. Further partitioning (ie setting everything to 'month' when not needed to keep the partitions below a few hundred thousand rows) can be detrimental.","title":"PGStac Partitioning"},{"location":"pgstac/#pgstac-indexes-queryables","text":"By default, PGStac includes indexes on the id, datetime, collection, geometry, and the eo:cloud_cover property. Further indexing can be added for additional properties globally or only on particular collections by modifications to the queryables table. Currently indexing is the only place the queryables table is used, but in future versions, it will be extended to provide a queryables backend api. To add a new global index across all partitions: INSERT INTO pgstac . queryables ( name , property_wrapper , property_index_type ) VALUES ( < property name > , < property wrapper > , < index type > ); Property wrapper should be one of to_int, to_float, to_tstz, or to_text. The index type should almost always be 'BTREE', but can be any PostgreSQL index type valid for the data type. More indexes is note necessarily better. You should only index the primary fields that are actively being used to search. Adding too many indexes can be very detrimental to performance and ingest speed. If your primary use case is delivering items sorted by datetime and you do not use the context extension, you likely will not need any further indexes.","title":"PGStac Indexes / Queryables"},{"location":"pypgstac/","text":"PgSTAC includes a Python utility for bulk data loading and managing migrations. PyPGStac is available on PyPI pip install pypgstac By default, PyPGStac does not install the psycopg dependency. If you want the database driver installed, use: pip install pypgstac[psycopg] Or can be built locally git clone https://github.com/stac-utils/pgstac cd pgstac/pypgstac pip install . pypgstac --help Usage: pypgstac [OPTIONS] COMMAND [ARGS]... Options: --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: initversion Get initial version. load Load STAC data into a pgstac database. migrate Migrate a pgstac database. pgready Wait for a pgstac database to accept connections. version Get version from a pgstac database. PyPGStac will get the database connection settings from the standard PG environment variables : PGHOST=0.0.0.0 PGPORT=5432 PGUSER=username PGDATABASE=postgis PGPASSWORD=asupersecretpassword It can also take a DSN database url \"postgresql://...\" via the --dsn flag. Migrations \u00b6 PyPGStac has a utility to help apply migrations to an existing PGStac instance to bring it up to date. There are two types of migrations: - Base migrations install PGStac into a database with no current PGStac installation. These migrations follow the file pattern \"pgstac.[version].sql\" - Incremental migrations are used to move PGStac from one version to the next. These migrations follow the file pattern \"pgstac.[version].[fromversion].sql\" Migrations are stored in pypgstac/pypgstac/migration`s and are distributed with the PyPGStac package. Running Migrations \u00b6 PyPGStac has a utility for checking the version of an existing PGStac database and applying the appropriate migrations in the correct order. It can also be used to setup a database from scratch. To create an initial PGStac database or bring an existing one up to date, check you have the pypgstac version installed you want to migrate to and run: pypgstac migrate Bulk Data Loading \u00b6 A python utility is included which allows to load data from any source openable by smart-open using python in a memory efficient streaming manner using PostgreSQL copy. There are options for collections and items and can be used either as a command line or a library. To load an ndjson of items directly using copy (will fail on any duplicate ids but is the fastest option to load new data you know will not conflict) pypgstac load items To load skipping any records that conflict with existing data pypgstac load items --method insert_ignore To upsert any records, adding anything new and replacing anything with the same id pypgstac load items --method upsert","title":"pyPgSTAC"},{"location":"pypgstac/#migrations","text":"PyPGStac has a utility to help apply migrations to an existing PGStac instance to bring it up to date. There are two types of migrations: - Base migrations install PGStac into a database with no current PGStac installation. These migrations follow the file pattern \"pgstac.[version].sql\" - Incremental migrations are used to move PGStac from one version to the next. These migrations follow the file pattern \"pgstac.[version].[fromversion].sql\" Migrations are stored in pypgstac/pypgstac/migration`s and are distributed with the PyPGStac package.","title":"Migrations"},{"location":"pypgstac/#running-migrations","text":"PyPGStac has a utility for checking the version of an existing PGStac database and applying the appropriate migrations in the correct order. It can also be used to setup a database from scratch. To create an initial PGStac database or bring an existing one up to date, check you have the pypgstac version installed you want to migrate to and run: pypgstac migrate","title":"Running Migrations"},{"location":"pypgstac/#bulk-data-loading","text":"A python utility is included which allows to load data from any source openable by smart-open using python in a memory efficient streaming manner using PostgreSQL copy. There are options for collections and items and can be used either as a command line or a library. To load an ndjson of items directly using copy (will fail on any duplicate ids but is the fastest option to load new data you know will not conflict) pypgstac load items To load skipping any records that conflict with existing data pypgstac load items --method insert_ignore To upsert any records, adding anything new and replacing anything with the same id pypgstac load items --method upsert","title":"Bulk Data Loading"},{"location":"release-notes/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning . v0.6.6 \u00b6 Added \u00b6 Add support for array operators in CQL2 (a_equals, a_contains, a_contained_by, a_overlaps). Add check in loader to make sure that pypgstac and pgstac versions match before loading data #123 v0.6.5 \u00b6 Fixed \u00b6 Fix for type casting when using the \"in\" operator #122 Fix failure of pypgstac load for large items #121 v0.6.4 \u00b6 Fixed \u00b6 Fixed casts for numeric data when a property is not in the queryables table to use the type from the incoming json filter Fixed issue loader grouping an unordered iterable by partition, speeding up loads of items with mixed partitions #116 v0.6.3 \u00b6 Fixed \u00b6 Fixed content_hydrate argument ordering which caused incorrect behavior in database hydration #115 Added \u00b6 Skip partition updates when unnecessary, which can drastically improve large ingest performance into existing partitions. #114 v0.6.2 \u00b6 Fixed \u00b6 Ensure special keys are not in content when loaded #112 v0.6.1 \u00b6 Fixed \u00b6 Fix issue where using equality operator against an array was only comparing the first element of the array v0.6.0 \u00b6 Fixed \u00b6 Fix function signatures for transactional functions (delete_item etc) to make sure that they are marked as volatile Fix function for getting start/end dates from a stac item Changed \u00b6 Update hydration/dehydration logic to make sure that it matches hydration/dehydration in pypgstac Update fields logic in pgstac to only use full paths and to match logic in stac-fastapi Always include id and collection on features regardless of fields setting Added \u00b6 Add tests to ensure that pgstac and pypgstac hydration logic is equivalent Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server. Add \"--dehydrated\" option to loader to be able to load a dehydrated file (or iterable) of items such as would be output using pg_dump or postgresql copy. Add \"--chunksize\" option to loader that can split the processing of an iterable or file into chunks of n records at a time v0.5.1 \u00b6 Fixed \u00b6 Changed \u00b6 Added \u00b6 Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server. v0.5.0 \u00b6 Version 0.5.0 is a major refactor of how data is stored. It is recommended to start a new database from scratch and to move data over rather than to use the inbuilt migration which will be very slow for larger amounts of data. Fixed \u00b6 Changed \u00b6 The partition layout has been changed from being hardcoded to a partition to week to using nested partitions. The first level is by collection, for each collection, there is an attribute partition_trunc which can be set to NULL (no temporal partitions), month, or year. CQL1 and Query Code have been refactored to translate to CQL2 to reduce duplicated code in query parsing. Unused functions have been stripped from the project. Pypgstac has been changed to use Fire rather than Typo. Pypgstac has been changed to use Psycopg3 rather than Asyncpg to enable easier use as both sync and async. Indexing has been reworked to eliminate indexes that from logs were not being used. The global json index on properties has been removed. Indexes on individual properties can be added either globally or per collection using the new queryables table. Triggers for maintaining partitions have been updated to reduce lock contention and to reflect the new data layout. The data pager which optimizes \"order by datetime\" searches has been updated to get time periods from the new partition layout and partition metadata. Tests have been updated to reflect the many changes. Added \u00b6 On ingest, the content in an item is compared to the metadata available at the collection level and duplicate information is stripped out (this is primarily data in the item_assets property). Logic is added in to merge this data back in on data usage. v0.4.5 \u00b6 Fixed \u00b6 Fixes support for using the intersects parameter at the base of a search (regression from changes in 0.4.4) Fixes issue where results for a search on id returned [None] rather than [] (regression from changes in 0.4.4) Changed \u00b6 Changes requirement for PostgreSQL to 13+, the triggers used to main partitions are not available to be used on partitions prior to 13 ( #90 ) Bump requirement for asyncpg to 0.25.0 ( #82 ) Added \u00b6 Added more tests. v0.4.4 \u00b6 Added \u00b6 Adds support for using ids, collections, datetime, bbox, and intersects parameters separated from the filter-lang (Fixes #85) Previously use of these parameters was translated into cql-json and then to SQL, so was not available when using cql2-json The deprecated query parameter is still only available when filter-lang is set to cql-json Changed \u00b6 Add PLPGSQL for item lookups by id so that the query plan for the simple query can be cached Use item_by_id function when looking up records used for paging filters Add a short circuit to search to use item_by_id lookup when using the ids parameter This short circuit avoids using the query cache for this simple case Ordering when using the ids parameter is hard coded to return results in the same order as the array passed in (this avoids the overhead of full parsing and additional overhead to sort) Fixed \u00b6 Fix to make sure that filtering on the search_wheres table leverages the functional index on the hash of the query rather than on the query itself. v0.4.3 \u00b6 Fixed \u00b6 Fix for optimization when using equals with json properties. Allow optimization for both \"eq\" and \"=\" (was only previously enabled for \"eq\") v0.4.2 \u00b6 Changed \u00b6 Add support for updated CQL2 spec to use timestamp or interval key Fixed \u00b6 Fix for 0.3.4 -> 0.3.5 migration making sure that partitions get renamed correctly v0.4.1 \u00b6 Changed \u00b6 Update typer to 0.4.0 to avoid clashes with click ( #76 ) Fixed \u00b6 Fix logic in getting settings to make sure that filter-lang set on query is respected. ( #77 ) Fix for large queries in the query cache. ( #71 ) v0.4.0 \u00b6 Fixed \u00b6 Fixes syntax for IN, BETWEEN, ISNULL, and NOT in CQL 1 ( #69 ) Added \u00b6 Adds support for modifying settings through pgstac_settings table and by passing in 'conf' object in search json to support AWS RDS where custom user configuration settings are not allowed and changing settings on the fly for a given query. Adds support for CQL2-JSON ( #67 ) Adds tests for all examples in github.com/radiantearth/stac-api-spec/blob/f5da775080ff3ff46d454c2888b6e796ee956faf/fragments/filter/README.md filter-lang parameter controls which dialect of CQL to use Adds 'default-filter-lang' setting to control what dialect to use when 'filter-lang' is not present old style stac 'query' object and top level ids, collections, datetime, bbox, and intersects parameters are only available with cql-json v0.3.4 \u00b6 Added \u00b6 add geometrysearch , geojsonsearch and xyzsearch for optimized searches for tiled requets ( #39 ) add create_items and upsert_items methods for bulk insert ( #39 ) v0.3.3 \u00b6 Fixed \u00b6 Fixed CQL term to be \"id\", not \"ids\" ( #46 ) Make sure featureCollection response has empty features [] not null ( #46 ) Fixed bugs for sortby and pagination ( #46 ) Make sure pgtap errors get caught in CI ( #46 ) v0.3.2 \u00b6 Fixed \u00b6 Fixed CQL term to be \"collections\", not \"collection\" ( #43 ) v0.3.1 \u00b6 TODO v0.2.8 \u00b6 Added \u00b6 Type hints to pypgstac that pass mypy checks ( #18 ) Fixed \u00b6 Fixed issue with pypgstac loads which caused some writes to fail ( #18 )","title":"Release Notes"},{"location":"release-notes/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"release-notes/#v066","text":"","title":"v0.6.6"},{"location":"release-notes/#added","text":"Add support for array operators in CQL2 (a_equals, a_contains, a_contained_by, a_overlaps). Add check in loader to make sure that pypgstac and pgstac versions match before loading data #123","title":"Added"},{"location":"release-notes/#v065","text":"","title":"v0.6.5"},{"location":"release-notes/#fixed","text":"Fix for type casting when using the \"in\" operator #122 Fix failure of pypgstac load for large items #121","title":"Fixed"},{"location":"release-notes/#v064","text":"","title":"v0.6.4"},{"location":"release-notes/#fixed_1","text":"Fixed casts for numeric data when a property is not in the queryables table to use the type from the incoming json filter Fixed issue loader grouping an unordered iterable by partition, speeding up loads of items with mixed partitions #116","title":"Fixed"},{"location":"release-notes/#v063","text":"","title":"v0.6.3"},{"location":"release-notes/#fixed_2","text":"Fixed content_hydrate argument ordering which caused incorrect behavior in database hydration #115","title":"Fixed"},{"location":"release-notes/#added_1","text":"Skip partition updates when unnecessary, which can drastically improve large ingest performance into existing partitions. #114","title":"Added"},{"location":"release-notes/#v062","text":"","title":"v0.6.2"},{"location":"release-notes/#fixed_3","text":"Ensure special keys are not in content when loaded #112","title":"Fixed"},{"location":"release-notes/#v061","text":"","title":"v0.6.1"},{"location":"release-notes/#fixed_4","text":"Fix issue where using equality operator against an array was only comparing the first element of the array","title":"Fixed"},{"location":"release-notes/#v060","text":"","title":"v0.6.0"},{"location":"release-notes/#fixed_5","text":"Fix function signatures for transactional functions (delete_item etc) to make sure that they are marked as volatile Fix function for getting start/end dates from a stac item","title":"Fixed"},{"location":"release-notes/#changed","text":"Update hydration/dehydration logic to make sure that it matches hydration/dehydration in pypgstac Update fields logic in pgstac to only use full paths and to match logic in stac-fastapi Always include id and collection on features regardless of fields setting","title":"Changed"},{"location":"release-notes/#added_2","text":"Add tests to ensure that pgstac and pypgstac hydration logic is equivalent Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server. Add \"--dehydrated\" option to loader to be able to load a dehydrated file (or iterable) of items such as would be output using pg_dump or postgresql copy. Add \"--chunksize\" option to loader that can split the processing of an iterable or file into chunks of n records at a time","title":"Added"},{"location":"release-notes/#v051","text":"","title":"v0.5.1"},{"location":"release-notes/#fixed_6","text":"","title":"Fixed"},{"location":"release-notes/#changed_1","text":"","title":"Changed"},{"location":"release-notes/#added_3","text":"Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server.","title":"Added"},{"location":"release-notes/#v050","text":"Version 0.5.0 is a major refactor of how data is stored. It is recommended to start a new database from scratch and to move data over rather than to use the inbuilt migration which will be very slow for larger amounts of data.","title":"v0.5.0"},{"location":"release-notes/#fixed_7","text":"","title":"Fixed"},{"location":"release-notes/#changed_2","text":"The partition layout has been changed from being hardcoded to a partition to week to using nested partitions. The first level is by collection, for each collection, there is an attribute partition_trunc which can be set to NULL (no temporal partitions), month, or year. CQL1 and Query Code have been refactored to translate to CQL2 to reduce duplicated code in query parsing. Unused functions have been stripped from the project. Pypgstac has been changed to use Fire rather than Typo. Pypgstac has been changed to use Psycopg3 rather than Asyncpg to enable easier use as both sync and async. Indexing has been reworked to eliminate indexes that from logs were not being used. The global json index on properties has been removed. Indexes on individual properties can be added either globally or per collection using the new queryables table. Triggers for maintaining partitions have been updated to reduce lock contention and to reflect the new data layout. The data pager which optimizes \"order by datetime\" searches has been updated to get time periods from the new partition layout and partition metadata. Tests have been updated to reflect the many changes.","title":"Changed"},{"location":"release-notes/#added_4","text":"On ingest, the content in an item is compared to the metadata available at the collection level and duplicate information is stripped out (this is primarily data in the item_assets property). Logic is added in to merge this data back in on data usage.","title":"Added"},{"location":"release-notes/#v045","text":"","title":"v0.4.5"},{"location":"release-notes/#fixed_8","text":"Fixes support for using the intersects parameter at the base of a search (regression from changes in 0.4.4) Fixes issue where results for a search on id returned [None] rather than [] (regression from changes in 0.4.4)","title":"Fixed"},{"location":"release-notes/#changed_3","text":"Changes requirement for PostgreSQL to 13+, the triggers used to main partitions are not available to be used on partitions prior to 13 ( #90 ) Bump requirement for asyncpg to 0.25.0 ( #82 )","title":"Changed"},{"location":"release-notes/#added_5","text":"Added more tests.","title":"Added"},{"location":"release-notes/#v044","text":"","title":"v0.4.4"},{"location":"release-notes/#added_6","text":"Adds support for using ids, collections, datetime, bbox, and intersects parameters separated from the filter-lang (Fixes #85) Previously use of these parameters was translated into cql-json and then to SQL, so was not available when using cql2-json The deprecated query parameter is still only available when filter-lang is set to cql-json","title":"Added"},{"location":"release-notes/#changed_4","text":"Add PLPGSQL for item lookups by id so that the query plan for the simple query can be cached Use item_by_id function when looking up records used for paging filters Add a short circuit to search to use item_by_id lookup when using the ids parameter This short circuit avoids using the query cache for this simple case Ordering when using the ids parameter is hard coded to return results in the same order as the array passed in (this avoids the overhead of full parsing and additional overhead to sort)","title":"Changed"},{"location":"release-notes/#fixed_9","text":"Fix to make sure that filtering on the search_wheres table leverages the functional index on the hash of the query rather than on the query itself.","title":"Fixed"},{"location":"release-notes/#v043","text":"","title":"v0.4.3"},{"location":"release-notes/#fixed_10","text":"Fix for optimization when using equals with json properties. Allow optimization for both \"eq\" and \"=\" (was only previously enabled for \"eq\")","title":"Fixed"},{"location":"release-notes/#v042","text":"","title":"v0.4.2"},{"location":"release-notes/#changed_5","text":"Add support for updated CQL2 spec to use timestamp or interval key","title":"Changed"},{"location":"release-notes/#fixed_11","text":"Fix for 0.3.4 -> 0.3.5 migration making sure that partitions get renamed correctly","title":"Fixed"},{"location":"release-notes/#v041","text":"","title":"v0.4.1"},{"location":"release-notes/#changed_6","text":"Update typer to 0.4.0 to avoid clashes with click ( #76 )","title":"Changed"},{"location":"release-notes/#fixed_12","text":"Fix logic in getting settings to make sure that filter-lang set on query is respected. ( #77 ) Fix for large queries in the query cache. ( #71 )","title":"Fixed"},{"location":"release-notes/#v040","text":"","title":"v0.4.0"},{"location":"release-notes/#fixed_13","text":"Fixes syntax for IN, BETWEEN, ISNULL, and NOT in CQL 1 ( #69 )","title":"Fixed"},{"location":"release-notes/#added_7","text":"Adds support for modifying settings through pgstac_settings table and by passing in 'conf' object in search json to support AWS RDS where custom user configuration settings are not allowed and changing settings on the fly for a given query. Adds support for CQL2-JSON ( #67 ) Adds tests for all examples in github.com/radiantearth/stac-api-spec/blob/f5da775080ff3ff46d454c2888b6e796ee956faf/fragments/filter/README.md filter-lang parameter controls which dialect of CQL to use Adds 'default-filter-lang' setting to control what dialect to use when 'filter-lang' is not present old style stac 'query' object and top level ids, collections, datetime, bbox, and intersects parameters are only available with cql-json","title":"Added"},{"location":"release-notes/#v034","text":"","title":"v0.3.4"},{"location":"release-notes/#added_8","text":"add geometrysearch , geojsonsearch and xyzsearch for optimized searches for tiled requets ( #39 ) add create_items and upsert_items methods for bulk insert ( #39 )","title":"Added"},{"location":"release-notes/#v033","text":"","title":"v0.3.3"},{"location":"release-notes/#fixed_14","text":"Fixed CQL term to be \"id\", not \"ids\" ( #46 ) Make sure featureCollection response has empty features [] not null ( #46 ) Fixed bugs for sortby and pagination ( #46 ) Make sure pgtap errors get caught in CI ( #46 )","title":"Fixed"},{"location":"release-notes/#v032","text":"","title":"v0.3.2"},{"location":"release-notes/#fixed_15","text":"Fixed CQL term to be \"collections\", not \"collection\" ( #43 )","title":"Fixed"},{"location":"release-notes/#v031","text":"TODO","title":"v0.3.1"},{"location":"release-notes/#v028","text":"","title":"v0.2.8"},{"location":"release-notes/#added_9","text":"Type hints to pypgstac that pass mypy checks ( #18 )","title":"Added"},{"location":"release-notes/#fixed_16","text":"Fixed issue with pypgstac loads which caused some writes to fail ( #18 )","title":"Fixed"}]}